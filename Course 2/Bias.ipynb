{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "4NUZBGun9FJI",
        "outputId": "498ef9c3-167c-4305-e7d3-208e630b62a9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2c55fb59d506>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Access the values of the arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bias\n",
        "\n",
        "### Goals\n",
        "In this notebook, you're going to explore a way to identify some biases of a GAN using a classifier, in a way that's well-suited for attempting to make a model independent of an input. Note that not all biases are as obvious as the ones you will see here.\n",
        "\n",
        "### Learning Objectives\n",
        "1.  Be able to distinguish a few different kinds of bias in terms of demographic parity, equality of odds, and equality of opportunity (as proposed [here](http://m-mitchell.com/papers/Adversarial_Bias_Mitigation.pdf)).\n",
        "2. Be able to use a classifier to try and detect biases in a GAN by analyzing the generator's implicit associations."
      ],
      "metadata": {
        "id": "DSSHFtpL_6Od"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenges\n",
        "\n",
        "One major challenge in assessing bias in GANs is that you still want your generator to be able to generate examples of different values of a protected class—the class you would like to mitigate bias against. While a classifier can be optimized to have its output be independent of a protected class, a generator which generates faces should be able to generate examples of various protected class values. \n",
        "\n",
        "When you generate examples with various values of a protected class, you don’t want those examples to correspond to any properties that aren’t strictly a function of that protected class. This is made especially difficult since many protected classes (e.g. gender or ethnicity) are social constructs, and what properties count as “a function of that protected class” will vary depending on who you ask. It’s certainly a hard balance to strike.\n",
        "\n",
        "Moreover, a protected class is rarely used to condition a GAN explicitly, so it is often necessary to resort to somewhat post-hoc methods (e.g. using a classifier trained on relevant features, which might be biased itself). \n",
        "\n",
        "In this assignment, you will learn one approach to detect potential bias, by analyzing correlations in feature classifications on the generated images. "
      ],
      "metadata": {
        "id": "dXwUXcAj__RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.datasets import CelebA\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(0) # Set for our testing purposes, please do not change!\n",
        "\n",
        "def show_tensor_images(image_tensor, num_images=16, size=(3, 64, 64), nrow=3):\n",
        "    '''\n",
        "    Function for visualizing images: Given a tensor of images, number of images,\n",
        "    size per image, and images per row, plots and prints the images in an uniform grid.\n",
        "    '''\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ulIBanX2AC_g"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    '''\n",
        "    Generator Class\n",
        "    Values:\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
        "              (CelebA is rgb, so 3 is your default)\n",
        "        hidden_dim: the inner dimension, a scalar\n",
        "    '''\n",
        "    def __init__(self, z_dim=10, im_chan=3, hidden_dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        # Build the neural network\n",
        "        self.gen = nn.Sequential(\n",
        "            self.make_gen_block(z_dim, hidden_dim * 8),\n",
        "            self.make_gen_block(hidden_dim * 8, hidden_dim * 4),\n",
        "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2),\n",
        "            self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
        "            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
        "        )\n",
        "\n",
        "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
        "        '''\n",
        "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
        "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
        "        Parameters:\n",
        "            input_channels: how many channels the input feature representation has\n",
        "            output_channels: how many channels the output feature representation should have\n",
        "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
        "            stride: the stride of the convolution\n",
        "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
        "                      (affects activation and batchnorm)\n",
        "        '''\n",
        "        if not final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.Tanh(),\n",
        "            )\n",
        "\n",
        "    def forward(self, noise):\n",
        "        '''\n",
        "        Function for completing a forward pass of the generator: Given a noise tensor, \n",
        "        returns generated images.\n",
        "        Parameters:\n",
        "            noise: a noise tensor with dimensions (n_samples, z_dim)\n",
        "        '''\n",
        "        x = noise.view(len(noise), self.z_dim, 1, 1)\n",
        "        return self.gen(x)\n",
        "\n",
        "def get_noise(n_samples, z_dim, device='cpu'):\n",
        "    '''\n",
        "    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)\n",
        "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
        "    Parameters:\n",
        "        n_samples: the number of samples to generate, a scalar\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        device: the device type\n",
        "    '''\n",
        "    return torch.randn(n_samples, z_dim, device=device)"
      ],
      "metadata": {
        "id": "u-R1b42nAJhs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    '''\n",
        "    Classifier Class\n",
        "    Values:\n",
        "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
        "              (CelebA is rgb, so 3 is your default)\n",
        "        n_classes: the total number of classes in the dataset, an integer scalar\n",
        "        hidden_dim: the inner dimension, a scalar\n",
        "    '''\n",
        "    def __init__(self, im_chan=3, n_classes=2, hidden_dim=64):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            self.make_classifier_block(im_chan, hidden_dim),\n",
        "            self.make_classifier_block(hidden_dim, hidden_dim * 2),\n",
        "            self.make_classifier_block(hidden_dim * 2, hidden_dim * 4, stride=3),\n",
        "            self.make_classifier_block(hidden_dim * 4, n_classes, final_layer=True),\n",
        "        )\n",
        "\n",
        "    def make_classifier_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
        "        '''\n",
        "        Function to return a sequence of operations corresponding to a classifier block; \n",
        "        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\n",
        "        Parameters:\n",
        "            input_channels: how many channels the input feature representation has\n",
        "            output_channels: how many channels the output feature representation should have\n",
        "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
        "            stride: the stride of the convolution\n",
        "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
        "                      (affects activation and batchnorm)\n",
        "        '''\n",
        "        if not final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "            )\n",
        "\n",
        "    def forward(self, image):\n",
        "        '''\n",
        "        Function for completing a forward pass of the classifier: Given an image tensor, \n",
        "        returns an n_classes-dimension tensor representing classes.\n",
        "        Parameters:\n",
        "            image: a flattened image tensor with im_chan channels\n",
        "        '''\n",
        "        class_pred = self.classifier(image)\n",
        "        return class_pred.view(len(class_pred), -1)"
      ],
      "metadata": {
        "id": "ki9UrJcoANcV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Specifying Parameters\n",
        "You will also need to specify a few parameters before you begin training:\n",
        "  *   z_dim: the dimension of the noise vector\n",
        "  *   batch_size: the number of images per forward/backward pass\n",
        "  *   device: the device type"
      ],
      "metadata": {
        "id": "2Hjq9U9KAQ6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z_dim = 64\n",
        "batch_size = 128\n",
        "device = 'cpu'"
      ],
      "metadata": {
        "id": "vvR2hJx1ASGJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can run this code to train your own classifier, but there is a provided pre-trained one \n",
        "# If you'd like to use this, just run \"train_classifier(filename)\"\n",
        "# To train and save a classifier on the label indices to that filename\n",
        "def train_classifier(filename):\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # You're going to target all the classes, so that's how many the classifier will learn\n",
        "    label_indices = range(40)\n",
        "\n",
        "    n_epochs = 3\n",
        "    display_step = 500\n",
        "    lr = 0.001\n",
        "    beta_1 = 0.5\n",
        "    beta_2 = 0.999\n",
        "    image_size = 64\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.CenterCrop(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        CelebA(\".\", split='train', download=True, transform=transform),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True)\n",
        "\n",
        "    classifier = Classifier(n_classes=len(label_indices)).to(device)\n",
        "    class_opt = torch.optim.Adam(classifier.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    cur_step = 0\n",
        "    classifier_losses = []\n",
        "    # classifier_val_losses = []\n",
        "    for epoch in range(n_epochs):\n",
        "        # Dataloader returns the batches\n",
        "        for real, labels in tqdm(dataloader):\n",
        "            real = real.to(device)\n",
        "            labels = labels[:, label_indices].to(device).float()\n",
        "\n",
        "            class_opt.zero_grad()\n",
        "            class_pred = classifier(real)\n",
        "            class_loss = criterion(class_pred, labels)\n",
        "            class_loss.backward() # Calculate the gradients\n",
        "            class_opt.step() # Update the weights\n",
        "            classifier_losses += [class_loss.item()] # Keep track of the average classifier loss\n",
        "\n",
        "            ### Visualization code ###\n",
        "            if cur_step % display_step == 0 and cur_step > 0:\n",
        "                class_mean = sum(classifier_losses[-display_step:]) / display_step\n",
        "                print(f\"Step {cur_step}: Classifier loss: {class_mean}\")\n",
        "                step_bins = 20\n",
        "                x_axis = sorted([i * step_bins for i in range(len(classifier_losses) // step_bins)] * step_bins)\n",
        "                sns.lineplot(x_axis, classifier_losses[:len(x_axis)], label=\"Classifier Loss\")\n",
        "                plt.legend()\n",
        "                plt.show()\n",
        "                torch.save({\"classifier\": classifier.state_dict()}, filename)\n",
        "            cur_step += 1\n",
        "\n",
        "# Uncomment the last line to train your own classfier - this line will not work in Coursera.\n",
        "# If you'd like to do this, you'll have to download it and run it, ideally using a GPU.\n",
        "# train_classifier(\"filename\")"
      ],
      "metadata": {
        "id": "sjNUheVaAWY4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading pretrained model\n",
        "import torch\n",
        "gen = Generator(z_dim).to(device)\n",
        "gen_dict = torch.load(\"pretrained_celeba.pth\", map_location=torch.device(device))[\"gen\"]\n",
        "gen.load_state_dict(gen_dict)\n",
        "gen.eval()\n",
        "\n",
        "n_classes = 40\n",
        "classifier = Classifier(n_classes=n_classes).to(device)\n",
        "class_dict = torch.load(\"pretrained_classifier.pth\", map_location=torch.device(device))[\"classifier\"]\n",
        "classifier.load_state_dict(class_dict)\n",
        "classifier.eval()\n",
        "print(\"Loaded the models!\")\n",
        "\n",
        "opt = torch.optim.Adam(classifier.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "ogxxflF-AaiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Correlation\n",
        "Now you can generate images using the generator. By also using the classifier, you will be generating images with different amounts of the \"male\" feature.\n",
        "\n",
        "You are welcome to experiment with other features as the target feature, but it is encouraged that you initially go through the notebook as is before exploring."
      ],
      "metadata": {
        "id": "fnkQDJBSAhVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First you generate a bunch of fake images with the generator\n",
        "n_images = 256\n",
        "fake_image_history = []\n",
        "classification_history = []\n",
        "grad_steps = 30 # How many gradient steps to take\n",
        "skip = 2 # How many gradient steps to skip in the visualization\n",
        "\n",
        "feature_names = [\"5oClockShadow\", \"ArchedEyebrows\", \"Attractive\", \"BagsUnderEyes\", \"Bald\", \"Bangs\",\n",
        "\"BigLips\", \"BigNose\", \"BlackHair\", \"BlondHair\", \"Blurry\", \"BrownHair\", \"BushyEyebrows\", \"Chubby\",\n",
        "\"DoubleChin\", \"Eyeglasses\", \"Goatee\", \"GrayHair\", \"HeavyMakeup\", \"HighCheekbones\", \"Male\", \n",
        "\"MouthSlightlyOpen\", \"Mustache\", \"NarrowEyes\", \"NoBeard\", \"OvalFace\", \"PaleSkin\", \"PointyNose\", \n",
        "\"RecedingHairline\", \"RosyCheeks\", \"Sideburn\", \"Smiling\", \"StraightHair\", \"WavyHair\", \"WearingEarrings\", \n",
        "\"WearingHat\", \"WearingLipstick\", \"WearingNecklace\", \"WearingNecktie\", \"Young\"]\n",
        "\n",
        "n_features = len(feature_names)\n",
        "# Set the target feature\n",
        "target_feature = \"Male\"\n",
        "target_indices = feature_names.index(target_feature)\n",
        "noise = get_noise(n_images, z_dim).to(device)\n",
        "new_noise = noise.clone().requires_grad_()\n",
        "starting_classifications = classifier(gen(new_noise)).cpu().detach()\n",
        "\n",
        "# Additive direction (more of a feature)\n",
        "for i in range(grad_steps):\n",
        "    opt.zero_grad()\n",
        "    fake = gen(new_noise)\n",
        "    fake_image_history += [fake]\n",
        "    classifications = classifier(fake)\n",
        "    classification_history += [classifications.cpu().detach()]\n",
        "    fake_classes = classifications[:, target_indices].mean()\n",
        "    fake_classes.backward()\n",
        "    new_noise.data += new_noise.grad / grad_steps\n",
        "\n",
        "# Subtractive direction (less of a feature)\n",
        "new_noise = noise.clone().requires_grad_()\n",
        "for i in range(grad_steps):\n",
        "    opt.zero_grad()\n",
        "    fake = gen(new_noise)\n",
        "    fake_image_history += [fake]\n",
        "    classifications = classifier(fake)\n",
        "    classification_history += [classifications.cpu().detach()]\n",
        "    fake_classes = classifications[:, target_indices].mean()\n",
        "    fake_classes.backward()\n",
        "    new_noise.data -= new_noise.grad / grad_steps\n",
        "\n",
        "classification_history = torch.stack(classification_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "OtW_2_g8Ai5l",
        "outputId": "f3d81413-532b-4f02-b09b-d61de982b5ce"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-88424e0629c6>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mnew_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mstarting_classifications\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Additive direction (more of a feature)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Correlation\n",
        "Now you can generate images using the generator. By also using the classifier, you will be generating images with different amounts of the \"male\" feature.\n",
        "\n",
        "You are welcome to experiment with other features as the target feature, but it is encouraged that you initially go through the notebook as is before exploring."
      ],
      "metadata": {
        "id": "OjFd9RqOApm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First you generate a bunch of fake images with the generator\n",
        "n_images = 256\n",
        "fake_image_history = []\n",
        "classification_history = []\n",
        "grad_steps = 30 # How many gradient steps to take\n",
        "skip = 2 # How many gradient steps to skip in the visualization\n",
        "\n",
        "feature_names = [\"5oClockShadow\", \"ArchedEyebrows\", \"Attractive\", \"BagsUnderEyes\", \"Bald\", \"Bangs\",\n",
        "\"BigLips\", \"BigNose\", \"BlackHair\", \"BlondHair\", \"Blurry\", \"BrownHair\", \"BushyEyebrows\", \"Chubby\",\n",
        "\"DoubleChin\", \"Eyeglasses\", \"Goatee\", \"GrayHair\", \"HeavyMakeup\", \"HighCheekbones\", \"Male\", \n",
        "\"MouthSlightlyOpen\", \"Mustache\", \"NarrowEyes\", \"NoBeard\", \"OvalFace\", \"PaleSkin\", \"PointyNose\", \n",
        "\"RecedingHairline\", \"RosyCheeks\", \"Sideburn\", \"Smiling\", \"StraightHair\", \"WavyHair\", \"WearingEarrings\", \n",
        "\"WearingHat\", \"WearingLipstick\", \"WearingNecklace\", \"WearingNecktie\", \"Young\"]\n",
        "\n",
        "n_features = len(feature_names)\n",
        "# Set the target feature\n",
        "target_feature = \"Male\"\n",
        "target_indices = feature_names.index(target_feature)\n",
        "noise = get_noise(n_images, z_dim).to(device)\n",
        "new_noise = noise.clone().requires_grad_()\n",
        "starting_classifications = classifier(gen(new_noise)).cpu().detach()\n",
        "\n",
        "# Additive direction (more of a feature)\n",
        "for i in range(grad_steps):\n",
        "    opt.zero_grad()\n",
        "    fake = gen(new_noise)\n",
        "    fake_image_history += [fake]\n",
        "    classifications = classifier(fake)\n",
        "    classification_history += [classifications.cpu().detach()]\n",
        "    fake_classes = classifications[:, target_indices].mean()\n",
        "    fake_classes.backward()\n",
        "    new_noise.data += new_noise.grad / grad_steps\n",
        "\n",
        "# Subtractive direction (less of a feature)\n",
        "new_noise = noise.clone().requires_grad_()\n",
        "for i in range(grad_steps):\n",
        "    opt.zero_grad()\n",
        "    fake = gen(new_noise)\n",
        "    fake_image_history += [fake]\n",
        "    classifications = classifier(fake)\n",
        "    classification_history += [classifications.cpu().detach()]\n",
        "    fake_classes = classifications[:, target_indices].mean()\n",
        "    fake_classes.backward()\n",
        "    new_noise.data -= new_noise.grad / grad_steps\n",
        "\n",
        "classification_history = torch.stack(classification_history)"
      ],
      "metadata": {
        "id": "4kY9AVIZAuR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You've now generated image samples, which have increasing or decreasing amounts of the target feature. You can visualize the way in which that affects other classified features. The x-axis will show you the amount of change in your target feature and the y-axis shows how much the other features change, as detected in those images by the classifier. Together, you will be able to see the covariance of \"male-ness\" and other features.\n",
        "\n",
        "You are started off with a set of features that have interesting associations with \"male-ness\", but you are welcome to change the features in `other_features` with others from `feature_names`."
      ],
      "metadata": {
        "id": "mKahK4XpAzeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "# Set the other features\n",
        "other_features = [\"Smiling\", \"Bald\", \"Young\", \"HeavyMakeup\", \"Attractive\"]\n",
        "classification_changes = (classification_history - starting_classifications[None, :, :]).numpy()\n",
        "for other_feature in other_features:\n",
        "    other_indices = feature_names.index(other_feature)\n",
        "    with sns.axes_style(\"darkgrid\"):\n",
        "        sns.regplot(\n",
        "            x=classification_changes[:, :, target_indices].reshape(-1), \n",
        "            y=classification_changes[:, :, other_indices].reshape(-1), \n",
        "            fit_reg=True,\n",
        "            truncate=True,\n",
        "            ci=99,\n",
        "            x_ci=99,\n",
        "            x_bins=len(classification_history),\n",
        "            label=other_feature\n",
        "        )\n",
        "plt.xlabel(target_feature)\n",
        "plt.ylabel(\"Other Feature\")\n",
        "plt.title(f\"Generator Biases: Features vs {target_feature}-ness\")\n",
        "plt.legend(loc=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EOBQhJncA2BO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This correlation detection can be used to reduce bias by penalizing this type of correlation in the loss during the training of the generator. However, currently there is no rigorous and accepted solution for debiasing GANs. A first step that you can take in the right direction comes before training the model: make sure that your dataset is inclusive and representative, and consider how you can mitigate the biases resulting from whatever data collection method you used—for example, getting a representative labelers for your task.\n",
        "\n",
        "It is important to note that, as highlighted in the lecture and by many researchers including Timnit Gebru and Emily Denton, a diverse dataset alone is not enough to eliminate bias. Even diverse datasets can reinforce existing structural biases by simply capturing common social biases. Mitigating these biases is an important and active area of research.\n",
        "\n",
        "### Note on CelebA\n",
        "You may have noticed that there are obvious correlations between the feature you are using, \"male\", and other seemingly unrelated features, \"smiling\" and \"young\" for example. This is because the CelebA dataset labels had no serious consideration for diversity. The data represents the biases of their labelers, the dataset creators, the social biases as a result of using a dataset based on American celebrities, and many others. Equipped with knowledge about bias, we trust that you will do better in the future datasets you create."
      ],
      "metadata": {
        "id": "Dfe7UcoXA6zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantification\n",
        "Finally, you can also quantitatively evaluate the degree to which these factors covary. Given a target index, for example corresponding to \"male,\" you'll want to return the other features that covary with that target feature the most. You'll want to account for both large negative and positive covariances, and you'll want to avoid returning the target feature in your list of covarying features (since a feature will often have a high covariance with itself). You'll complete some helper functions first, each of which should be one or two lines long."
      ],
      "metadata": {
        "id": "WGWL1O9bBA6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions import MultivariateNormal\n",
        "def covariance_matrix_from_examples(examples):\n",
        "    \"\"\"\n",
        "    Helper function for get_top_covariances to calculate a covariance matrix. \n",
        "    Parameter: examples: a list of steps corresponding to samples of shape (2 * grad_steps, n_images, n_features)\n",
        "    Returns: the (n_features, n_features) covariance matrix from the examples\n",
        "    \"\"\"\n",
        "    # Hint: np.cov will be useful here - note the rowvar argument!\n",
        "    ### START CODE HERE ###\n",
        "    return np.cov(examples.reshape(-1, examples.shape[-1]), rowvar = False)\n",
        "    ### END CODE HERE ###\n"
      ],
      "metadata": {
        "id": "EI8V3tlRBE6F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you'll write a helper function to return the indices of a numpy array in order of magnitude."
      ],
      "metadata": {
        "id": "7IkyUt3aBK_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_magnitude_indices(values):\n",
        "    \"\"\"\n",
        "    Helper function for get_top_covariances to get indices by magnitude. \n",
        "    Parameter: values, a list of values as a numpy array of shape (n_values)\n",
        "    Returns: numpy array of indices sorted from greatest to least by the magnitudes of their corresponding values\n",
        "    \"\"\"\n",
        "    # Hint: This can be done in one or two lines using np.argsort and np.abs!\n",
        "    ### START CODE HERE ###\n",
        "    return np.argsort(-1*np.abs(values))\n",
        "    ### END CODE HERE ###\n",
        "    return top_indices"
      ],
      "metadata": {
        "id": "QIj0Kd94BLrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you'll write a helper function to return a list with an element removed by the value, in an unchanged order. In this case, you won't have to remove any values multiple times, so don't worry about how you handle multiple examples."
      ],
      "metadata": {
        "id": "l2r0XWneBO5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_from_list(indices, index_to_remove):\n",
        "    \"\"\"\n",
        "    Helper function for get_top_covariances to remove an index from an array. \n",
        "    Parameter: indices, a list of indices as a numpy array of shape (n_indices)\n",
        "    Returns: the numpy array of indices in the same order without index_to_remove\n",
        "    \"\"\"\n",
        "    # Hint: There are many ways to do this, but please don't edit the list in-place.\n",
        "    # If you're not very familiar with array indexing, you may find this page helpful:\n",
        "    # https://numpy.org/devdocs/reference/arrays.indexing.html (especially boolean indexing)\n",
        "    ### START CODE HERE ###\n",
        "    return indices[np.where(indices != index_to_remove)[0]]\n",
        "    ### END CODE HERE ###\n",
        "    return new_indices"
      ],
      "metadata": {
        "id": "j1H6LxEuBOBI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED CELL: get_top_covariances\n",
        "def get_top_covariances(classification_changes, target_index, top_n=10):\n",
        "    '''\n",
        "    Function for getting the top n covariances: Given a list of classification changes\n",
        "    and the index of the target feature, returns \n",
        "    (1) relevant_indices: a list or tensor (numpy or torch) of the indices corresponding \n",
        "        to the n features that covary most with the target in terms of absolute covariance\n",
        "    (2) highest_covariances: a list or tensor of the degrees to which they covary.\n",
        "    Parameters:\n",
        "        classification_changes: relative changes in classifications of each generated image \n",
        "          resulting from optimizing the target feature (see above for a visualization)\n",
        "        target_index: the index of the target feature, a scalar\n",
        "        top_n: the top most number of elements to return, default is 10\n",
        "    '''\n",
        "    # Hint: Don't forget you also care about negative covariances!\n",
        "    # Note that classification_changes has a shape of (2 * grad_steps, n_images, n_features) \n",
        "    # where n_features is the number of features measured by the classifier, and you are looking\n",
        "    # for the covariance of the features based on the (2 * grad_steps * n_images) samples.\n",
        "    #### START CODE HERE ####\n",
        "    Cov_Mat = covariance_matrix_from_examples(classification_changes)\n",
        "    relevant_indices = remove_from_list(get_top_magnitude_indices(Cov_Mat[:,target_index]), target_index)[:top_n]\n",
        "    highest_covariances = Cov_Mat[relevant_indices][:,target_index]\n",
        "    #### END CODE HERE ####\n",
        "    return relevant_indices, highest_covariances"
      ],
      "metadata": {
        "id": "eykS9Ns3BZY4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_indices, highest_covariances = get_top_covariances(classification_changes, target_indices, top_n=10)\n",
        "print(relevant_indices)\n",
        "assert relevant_indices[9] == 34\n",
        "assert len(relevant_indices) == 10\n",
        "assert highest_covariances[8] - (-1.2418) < 1e-3\n",
        "for index, covariance in zip(relevant_indices, highest_covariances):\n",
        "    print(f\"{feature_names[index]}  {covariance:f}\")"
      ],
      "metadata": {
        "id": "APXT0X83Bhtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YK4Ta7OnAhbr"
      }
    }
  ]
}