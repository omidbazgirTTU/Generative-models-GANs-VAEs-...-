{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Challenges With Evaluating GANs\n",
        "\n",
        "#### Loss is Uninformative of Performance\n",
        "One aspect that makes evaluating GANs challenging is that the loss tells us little about their performance. Unlike with classifiers, where a low loss on a test set indicates superior performance, a low loss for the generator or discriminator suggests that learning has stopped. \n",
        "\n",
        "\n",
        "#### No Clear Non-human Metric\n",
        "If you define the goal of a GAN as \"generating images which look real to people\" then it's technically possible to measure this directly: [you can ask people to act as a discriminator](https://arxiv.org/abs/1904.01121). However, this takes significant time and money so ideally you can use a proxy for this. There is also no \"perfect\" discriminator that can differentiate reals from fakes - if there were, a lot of machine learning tasks would be solved ;)\n",
        "\n",
        "In this notebook, you will implement Fréchet Inception Distance, one method which aims to solve these issues. "
      ],
      "metadata": {
        "id": "lRR44tiS4OIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CelebA\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(0) # Set for our testing purposes, please do not change!\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    '''\n",
        "    Generator Class\n",
        "    Values:\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
        "              (CelebA is rgb, so 3 is your default)\n",
        "        hidden_dim: the inner dimension, a scalar\n",
        "    '''\n",
        "    def __init__(self, z_dim=10, im_chan=3, hidden_dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        # Build the neural network\n",
        "        self.gen = nn.Sequential(\n",
        "            self.make_gen_block(z_dim, hidden_dim * 8),\n",
        "            self.make_gen_block(hidden_dim * 8, hidden_dim * 4),\n",
        "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2),\n",
        "            self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
        "            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
        "        )\n",
        "\n",
        "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
        "        '''\n",
        "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
        "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
        "        Parameters:\n",
        "            input_channels: how many channels the input feature representation has\n",
        "            output_channels: how many channels the output feature representation should have\n",
        "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
        "            stride: the stride of the convolution\n",
        "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
        "                      (affects activation and batchnorm)\n",
        "        '''\n",
        "        if not final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.Tanh(),\n",
        "            )\n",
        "\n",
        "    def forward(self, noise):\n",
        "        '''\n",
        "        Function for completing a forward pass of the generator: Given a noise tensor, \n",
        "        returns generated images.\n",
        "        Parameters:\n",
        "            noise: a noise tensor with dimensions (n_samples, z_dim)\n",
        "        '''\n",
        "        x = noise.view(len(noise), self.z_dim, 1, 1)\n",
        "        return self.gen(x)\n",
        "\n",
        "def get_noise(n_samples, z_dim, device='cpu'):\n",
        "    '''\n",
        "    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)\n",
        "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
        "    Parameters:\n",
        "        n_samples: the number of samples to generate, a scalar\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        device: the device type\n",
        "    '''\n",
        "    return torch.randn(n_samples, z_dim, device=device)"
      ],
      "metadata": {
        "id": "cGN5EQXB4Q9g"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Pre-trained Model\n",
        "\n",
        "Now, you can set the arguments for the model and load the dataset:\n",
        "  *   z_dim: the dimension of the noise vector\n",
        "  *   image_size: the image size of the input to Inception (more details in the following section)\n",
        "  *   device: the device type"
      ],
      "metadata": {
        "id": "kRFibef44WOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z_dim = 64\n",
        "image_size = 299\n",
        "device = 'cpu' # 'cuda'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "in_coursera = False # Set this to false if you're running this outside Coursera\n",
        "if in_coursera:\n",
        "    import numpy as np\n",
        "    data = torch.Tensor(np.load('fid_images_tensor.npz', allow_pickle=True)['arr_0'])\n",
        "    dataset = torch.utils.data.TensorDataset(data, data)\n",
        "else:\n",
        "    dataset = CelebA(\".\", download=True, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PpRO9fL4XmT",
        "outputId": "99dd1d36-798a-466f-9932-049bf9eac8f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1443490838it [00:13, 107776474.85it/s]\n",
            "26721026it [00:00, 104316910.14it/s]\n",
            "3424458it [00:00, 190759252.10it/s]\n",
            "6082035it [00:00, 54267846.61it/s]\n",
            "12156055it [00:00, 72245107.77it/s]\n",
            "2836386it [00:00, 117992037.23it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, you can load and initialize the model with weights from a pre-trained model. This allows you to use the pre-trained model as if you trained it yourself."
      ],
      "metadata": {
        "id": "8jCcjZU14kl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen = Generator(z_dim).to(device)\n",
        "gen.load_state_dict(torch.load(f\"pretrained_celeba.pth\", map_location=torch.device(device))[\"gen\"])\n",
        "gen = gen.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "6pVdhsP-4pdx",
        "outputId": "f345ce97-15a4-4897-ef5d-a8a8d4015ede"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1e15c6abc2d0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"pretrained_celeba.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gen\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pretrained_celeba.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inception-v3 Network\n",
        "Inception-V3 is a neural network trained on [ImageNet](http://www.image-net.org/) to classify objects. You may recall from the lectures that ImageNet has over 1 million images to train on. As a result, Inception-V3 does a good job detecting features and classifying images. Here, you will load Inception-V3 as `inception_model`.\n",
        "\n",
        "<!--  \n",
        "In the past, people would use a pretrained Inception network to identify the classes of the objects generated by a GAN and measure how similar the distribution of classes generated was to the true image (using KL divergence). This is known as inception score. \n",
        "\n",
        "However, there are many problems with this metric. Barratt and Sharma's 2018 \"[A Note on the Inception Score](https://arxiv.org/pdf/1801.01973.pdf)\" highlights many issues with this approach. Among them, they highlight its instability, its exploitability, and the widespread use of Inception Score on models not trained on ImageNet.  -->\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9LfDXJ7n4s0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import inception_v3\n",
        "inception_model = inception_v3(pretrained=False)\n",
        "inception_model.load_state_dict(torch.load(\"inception_v3_google-1a9a5a14.pth\"))\n",
        "inception_model.to(device)\n",
        "inception_model = inception_model.eval() # Evaluation mode"
      ],
      "metadata": {
        "id": "YT3o-dFy4vOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fréchet Inception Distance\n",
        "\n",
        "Fréchet Inception Distance (FID) was proposed as an improvement over Inception Score and still uses the Inception-v3 network as part of its calculation. However, instead of using the classification labels of the Inception-v3 network, it uses the output from an earlier layer—the layer right before the labels. This is often called the feature layer. Research has shown that deep convolutional neural networks trained on difficult tasks, like classifying many classes, build increasingly sophisticated representations of features going deeper into the network. For example, the first few layers may learn to detect different kinds of edges and curves, while the later layers may have neurons that fire in response to human faces.\n",
        "\n",
        "To get the feature layer of a convolutional neural network, you can replace the final fully connected layer with an identity layer that simply returns whatever input it received, unchanged. This essentially removes the final classification layer and leaves you with the intermediate outputs from the layer before.\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>\n",
        "<font size=\"3\" color=\"green\">\n",
        "<b>Optional hint for <code><font size=\"4\">inception_model.fc</font></code></b>\n",
        "</font>\n",
        "</summary>\n",
        "\n",
        "1.    You may find [torch.nn.Identity()](https://pytorch.org/docs/master/generated/torch.nn.Identity.html) helpful.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "f01A5JNe4xsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED CELL: inception_model.fc\n",
        "\n",
        "# You want to replace the final fully-connected (fc) layer \n",
        "# with an identity function layer to cut off the classification\n",
        "# layer and get a feature extractor\n",
        "#### START CODE HERE ####\n",
        "inception_model.fc = torch.nn.Identity()\n",
        "#### END CODE HERE ####"
      ],
      "metadata": {
        "id": "eO48aewp47Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fréchet Distance \n",
        "Fréchet distance uses the values from the feature layer for two sets of images, say reals and fakes, and compares different statistical properties between them to see how different they are. Specifically, Fréchet distance finds the shortest distance needed to walk along two lines, or two curves, simultaneously. The most intuitive explanation of Fréchet distance is as the \"minimum leash distance\" between two points. Imagine yourself and your dog, both moving along two curves. If you walked on one curve and your dog, attached to a leash, walked on the other at the same pace, what is the least amount of leash that you can give your dog so that you never need to give them more slack during your walk? Using this, the Fréchet distance measures the similarity between these two curves.\n",
        "\n",
        "The basic idea is similar for calculating the Fréchet distance between two probability distributions. You'll start by seeing what this looks like in one-dimensional, also called univariate, space."
      ],
      "metadata": {
        "id": "avnQR1XH5DHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Univariate Fréchet Distance\n",
        "You can calculate the distance between two normal distributions $X$ and $Y$ with means $\\mu_X$ and $\\mu_Y$ and standard deviations $\\sigma_X$ and $\\sigma_Y$, as:\n",
        "\n",
        "$$d(X,Y) = (\\mu_X-\\mu_Y)^2 + (\\sigma_X-\\sigma_Y)^2 $$\n",
        "\n",
        "Pretty simple, right? Now you can see how it can be converted to be used in multi-dimensional, which is also called multivariate, space."
      ],
      "metadata": {
        "id": "tT5Q09225Dzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multivariate Fréchet Distance\n",
        "**Covariance**\n",
        "\n",
        "To find the Fréchet distance between two multivariate normal distributions, you first need to find the covariance instead of the standard deviation. The covariance, which is the multivariate version of variance (the square of standard deviation), is represented using a square matrix where the side length is equal to the number of dimensions. Since the feature vectors you will be using have 2048 values/weights, the covariance matrix will be 2048 x 2048. But for the sake of an example, this is a covariance matrix in a two-dimensional space:\n",
        "\n",
        "$\\Sigma = \\left(\\begin{array}{cc} \n",
        "1 & 0\\\\ \n",
        "0 & 1\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "The value at location $(i, j)$ corresponds to the covariance of vector $i$ with vector $j$. Since the covariance of $i$ with $j$ and $j$ with $i$ are equivalent, the matrix will always be symmetric with respect to the diagonal. The diagonal is the covariance of that element with itself. In this example, there are zeros everywhere except the diagonal. That means that the two dimensions are independent of one another, they are completely unrelated.\n",
        "\n",
        "The following code cell will visualize this matrix."
      ],
      "metadata": {
        "id": "UiwIOL-55Hox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Formula**\n",
        "\n",
        "Based on the paper, \"[The Fréchet distance between multivariate normal distributions](https://core.ac.uk/reader/82269844)\" by Dowson and Landau (1982), the Fréchet distance between two multivariate normal distributions $X$ and $Y$ is:\n",
        "\n",
        "$d(X, Y) = \\Vert\\mu_X-\\mu_Y\\Vert^2 + \\mathrm{Tr}\\left(\\Sigma_X+\\Sigma_Y - 2 \\sqrt{\\Sigma_X \\Sigma_Y}\\right)$\n",
        "\n",
        "Similar to the formula for univariate Fréchet distance, you can calculate the distance between the means and the distance between the standard deviations. However, calculating the distance between the standard deviations changes slightly here, as it includes the matrix product and matrix square root. $\\mathrm{Tr}$ refers to the trace, the sum of the diagonal elements of a matrix.\n",
        "\n",
        "Now you can implement this!\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>\n",
        "<font size=\"3\" color=\"green\">\n",
        "<b>Optional hints for <code><font size=\"4\">frechet_distance</font></code></b>\n",
        "</font>\n",
        "</summary>\n",
        "\n",
        "1.   You want to implement the above equation in code.\n",
        "2.   You might find the functions `torch.norm` and `torch.trace` helpful here.\n",
        "3.   A matrix_sqrt function is defined for you above -- you need to use it instead of `torch.sqrt()` which only gets the elementwise square root instead of the matrix square root.\n",
        "4.   You can also use the `@` symbol for matrix multiplication.\n",
        "</details>"
      ],
      "metadata": {
        "id": "G9Y04qmt5Oo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "# This is the matrix square root function you will be using\n",
        "def matrix_sqrt(x):\n",
        "    '''\n",
        "    Function that takes in a matrix and returns the square root of that matrix.\n",
        "    For an input matrix A, the output matrix B would be such that B @ B is the matrix A.\n",
        "    Parameters:\n",
        "        x: a matrix\n",
        "    '''\n",
        "    y = x.cpu().detach().numpy()\n",
        "    y = scipy.linalg.sqrtm(y)\n",
        "    return torch.Tensor(y.real, device=x.device)"
      ],
      "metadata": {
        "id": "Wh5X6N_Q5QO7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: frechet_distance\n",
        "def frechet_distance(mu_x, mu_y, sigma_x, sigma_y):\n",
        "    '''\n",
        "    Function for returning the Fréchet distance between multivariate Gaussians,\n",
        "    parameterized by their means and covariance matrices.\n",
        "    Parameters:\n",
        "        mu_x: the mean of the first Gaussian, (n_features)\n",
        "        mu_y: the mean of the second Gaussian, (n_features) \n",
        "        sigma_x: the covariance matrix of the first Gaussian, (n_features, n_features)\n",
        "        sigma_y: the covariance matrix of the second Gaussian, (n_features, n_features)\n",
        "    '''\n",
        "    #### START CODE HERE ####\n",
        "    return torch.norm(mu_x - mu_y)**2 + torch.trace(sigma_x + sigma_y -2*matrix_sqrt(torch.mm(sigma_x,sigma_y)))\n",
        "    #### END CODE HERE ####"
      ],
      "metadata": {
        "id": "_v1S0imM5K9y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(img):\n",
        "    img = torch.nn.functional.interpolate(img, size=(299, 299), mode='bilinear', align_corners=False)\n",
        "    return img"
      ],
      "metadata": {
        "id": "FgnK2jXb5ZwL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def get_covariance(features):\n",
        "    return torch.Tensor(np.cov(features.detach().numpy(), rowvar=False))"
      ],
      "metadata": {
        "id": "LIgR9T9d5Zx_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, you can use the pre-trained Inception-v3 model to compute features of the real and fake images. With these features, you can then get the covariance and means of these features across many samples. \n",
        "\n",
        "First, you get the features of the real and fake images using the Inception-v3 model:"
      ],
      "metadata": {
        "id": "D4wv2es75eb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fake_features_list = []\n",
        "real_features_list = []\n",
        "\n",
        "gen.eval()\n",
        "n_samples = 512 # The total number of samples\n",
        "batch_size = 4 # Samples per iteration\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True)\n",
        "\n",
        "cur_samples = 0\n",
        "with torch.no_grad(): # You don't need to calculate gradients here, so you do this to save memory\n",
        "    try:\n",
        "        for real_example, _ in tqdm(dataloader, total=n_samples // batch_size): # Go by batch\n",
        "            real_samples = real_example\n",
        "            real_features = inception_model(real_samples.to(device)).detach().to('cpu') # Move features to CPU\n",
        "            real_features_list.append(real_features)\n",
        "\n",
        "            fake_samples = get_noise(len(real_example), z_dim).to(device)\n",
        "            fake_samples = preprocess(gen(fake_samples))\n",
        "            fake_features = inception_model(fake_samples.to(device)).detach().to('cpu')\n",
        "            fake_features_list.append(fake_features)\n",
        "            cur_samples += len(real_samples)\n",
        "            if cur_samples > n_samples:\n",
        "                break\n",
        "    except:\n",
        "        print(\"Error in loop\")"
      ],
      "metadata": {
        "id": "qRUST0cU5gWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# UNIT TEST COMMENT: Needed as is for autograding\n",
        "fake_features_all = torch.cat(fake_features_list)\n",
        "real_features_all = torch.cat(real_features_list)"
      ],
      "metadata": {
        "id": "3ZkBA6Vd5lwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED CELL\n",
        "\n",
        "# Calculate the covariance matrix for the fake and real features\n",
        "# and also calculate the means of the feature over the batch (for each feature dimension mean)\n",
        "#### START CODE HERE ####\n",
        "mu_fake = fake_features_all.mean(axis = 0)\n",
        "mu_real = real_features_all.mean(axis = 0)\n",
        "sigma_fake = get_covariance(fake_features_all)\n",
        "sigma_real = get_covariance(real_features_all)\n",
        "#### END CODE HERE ####"
      ],
      "metadata": {
        "id": "6ooki-Ec5mYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vaisualization of pairwise features\n",
        "indices = [2, 4, 5]\n",
        "fake_dist = MultivariateNormal(mu_fake[indices], sigma_fake[indices][:, indices])\n",
        "fake_samples = fake_dist.sample((5000,))\n",
        "real_dist = MultivariateNormal(mu_real[indices], sigma_real[indices][:, indices])\n",
        "real_samples = real_dist.sample((5000,))\n",
        "\n",
        "import pandas as pd\n",
        "df_fake = pd.DataFrame(fake_samples.numpy(), columns=indices)\n",
        "df_real = pd.DataFrame(real_samples.numpy(), columns=indices)\n",
        "df_fake[\"is_real\"] = \"no\"\n",
        "df_real[\"is_real\"] = \"yes\"\n",
        "df = pd.concat([df_fake, df_real])\n",
        "sns.pairplot(data = df, plot_kws={'alpha': 0.1}, hue='is_real')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2VSlGkGi5r6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, you can use your earlier `frechet_distance` function to calculate the FID and evaluate your GAN. You can see how similar/different the features of the generated images are to the features of the real images. The next cell might take five minutes or so to run in Coursera."
      ],
      "metadata": {
        "id": "_fK30AHj5z93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    print(frechet_distance(mu_real, mu_fake, sigma_real, sigma_fake).item())"
      ],
      "metadata": {
        "id": "TH4sAhlK5zRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Y_0D6Ydc4-BI"
      }
    }
  ]
}