{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sumV6sb1ENdg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# InfoGAN\n",
        "\n",
        "We're going to learn about InfoGAN in order to generate disentangled outputs, based on the paper, [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657) by Chen et. al. While there are many approaches to disentanglement, this is one of the more widely used and better known. \n",
        "\n",
        "InfoGAN can be understood like this: you want to separate your model into two parts: $z$, corresponding to truly random noise, and $c$ corresponding to the \"latent code.\" The latent code $c$ which can be thought of as a \"hidden\" condition in a conditional generator, and you'd like it to have an interpretable meaning. \n",
        "\n",
        "Now, you'll likely immediately wonder, how do they get $c$, which is just some random set of numbers, to be more interpretable than any dimension in a typical GAN? The answer is \"mutual information\": essentially, you would like each dimension of the latent code to be as obvious a function as possible of the generated images. Read on for a more thorough theoretical and practical treatment."
      ],
      "metadata": {
        "id": "BiB_XEzoEU_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Formally: Variational Lower Bound\n",
        "The [information entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) ${H} (X)=-\\sum _{i=1}^{n}{P(x_{i})\\log P (x_{i})}$\n",
        "can be understood to the amount of \"information\" in the distribution $X$. For example, the information entropy of $n$ fair coins is $n$ bits. You've also seen a similar equation before: the cross-entropy loss. Moreover, mutual information $I(X;Y) = H(X) - H(X\\vert Y)$, which the authors of InfoGAN describe as (intuitively) the \"reduction of uncertainty in $X$ when $Y$ is observed.\" \n",
        "\n",
        "In InfoGAN, you'd like to maximize $I(c; G(z, c))$, the mutual information between the latent code $c$ and the generated images $G(z, c)$.  Since it's difficult to know $P(c | G(z, c))$, you add a second output to the discriminator to predict $P(c | G(z, c))$. \n",
        "\n",
        "Let $\\Delta = D_{KL}(P(\\cdot|x) \\Vert Q(\\cdot|x))$, the [Kullback-Leibler_divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the true and approximate distribution. Then, based on Equation 4 in the paper, the mutual information has the following lower bound: \n",
        "$$\\begin{split}\n",
        "I(c; G(z, c)) & = H(c) - H(c|G(z, c)) \\\\\n",
        "& = {\\mathbb{E}}_{x \\sim G(z, c)} [ {\\mathbb{E}}_{c' \\sim P(c, x)} \\log P(c' | x) ] + H(c) \\textit{ (by definition of H)}\\\\\n",
        "& = {\\mathbb{E}}_{x \\sim G(z, c)} [\\Delta + {\\mathbb{E}}_{c' \\sim P(c, x)} \\log Q(c' | x) ] + H(c) \\textit{ (approximation error)}\\\\\n",
        "& \\geq {\\mathbb{E}}_{x \\sim G(z, c)} [{\\mathbb{E}}_{c' \\sim P(c, x)} \\log Q(c' | x) ] + H(c) \\textit{ (KL divergence is non-negative)}\\\\\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "For a given latent code distribution, $H(c)$ is fixed, so the following makes a good loss:\n",
        "\n",
        "$${\\mathbb{E}}_{x \\sim G(z, c)} [{\\mathbb{E}}_{c' \\sim P(c, x)} \\log Q(c' | x) ]$$\n",
        "\n",
        "Which is the mean cross entropy loss of the approximation over the generator's images. "
      ],
      "metadata": {
        "id": "yB1ksif2ElW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Updating the Minimax Game\n",
        "\n",
        "A vanilla generator and discriminator follow a minimax game: $\\displaystyle \\min_{G} \\max_{D} V(D, G) = \\mathbb{E}(\\log D(x)) + \\mathbb{E}(\\log (1 - D(G(z))))$.\n",
        "\n",
        "To encourage mutual information, this game is updated for $Q$ to maximize mutual information: $\\displaystyle \\min_{G, Q} \\max_{D} V(D, G) - \\lambda I(c; G(z, c))$"
      ],
      "metadata": {
        "id": "dVey93WfEqNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing InfoGAN\n",
        "\n",
        "For this notebook, you'll be using the MNIST dataset again. \n",
        "\n",
        "You will begin by importing the necessary libraries and building the generator and discriminator. The generator will be the same as before, but the discriminator will be modified with more dimensions in its output."
      ],
      "metadata": {
        "id": "pbOhmXDFEtgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(0) # Set for our testing purposes, please do not change!\n",
        "\n",
        "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28), nrow=5, show=True):\n",
        "    '''\n",
        "    Function for visualizing images: Given a tensor of images, number of images, and\n",
        "    size per image, plots and prints the images in an uniform grid.\n",
        "    '''\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    if show:\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "j-Px0JTREwoQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator and Noise\n",
        "class Generator(nn.Module):\n",
        "    '''\n",
        "    Generator Class\n",
        "    Values:\n",
        "        input_dim: the dimension of the input vector, a scalar\n",
        "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
        "              (MNIST is black-and-white, so 1 channel is your default)\n",
        "        hidden_dim: the inner dimension, a scalar\n",
        "    '''\n",
        "    def __init__(self, input_dim=10, im_chan=1, hidden_dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        # Build the neural network\n",
        "        self.gen = nn.Sequential(\n",
        "            self.make_gen_block(input_dim, hidden_dim * 4),\n",
        "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n",
        "            self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
        "            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
        "        )\n",
        "\n",
        "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
        "        '''\n",
        "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
        "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
        "        Parameters:\n",
        "            input_channels: how many channels the input feature representation has\n",
        "            output_channels: how many channels the output feature representation should have\n",
        "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
        "            stride: the stride of the convolution\n",
        "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
        "                      (affects activation and batchnorm)\n",
        "        '''\n",
        "        if not final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.Tanh(),\n",
        "            )\n",
        "\n",
        "    def forward(self, noise):\n",
        "        '''\n",
        "        Function for completing a forward pass of the generator: Given a noise tensor, \n",
        "        returns generated images.\n",
        "        Parameters:\n",
        "            noise: a noise tensor with dimensions (n_samples, input_dim)\n",
        "        '''\n",
        "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
        "        return self.gen(x)\n",
        "\n",
        "def get_noise(n_samples, input_dim, device='cpu'):\n",
        "    '''\n",
        "    Function for creating noise vectors: Given the dimensions (n_samples, input_dim)\n",
        "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
        "    Parameters:\n",
        "        n_samples: the number of samples to generate, a scalar\n",
        "        input_dim: the dimension of the input vector, a scalar\n",
        "        device: the device type\n",
        "    '''\n",
        "    return torch.randn(n_samples, input_dim, device=device)"
      ],
      "metadata": {
        "id": "plYfQU9BEzV8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### InfoGAN Discriminator\n",
        "\n",
        "You update the final layer to predict a distribution for $c$ from $x$, alongside the traditional discriminator output. Since you're assuming a normal prior in this assignment, you output a mean and a log-variance prediction."
      ],
      "metadata": {
        "id": "LveNoYbCE5bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    '''\n",
        "    Discriminator Class\n",
        "    Values:\n",
        "      im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
        "            (MNIST is black-and-white, so 1 channel is your default)\n",
        "      hidden_dim: the inner dimension, a scalar\n",
        "      c_dim: the number of latent code dimensions - \n",
        "    '''\n",
        "    def __init__(self, im_chan=1, hidden_dim=64, c_dim=10):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            self.make_disc_block(im_chan, hidden_dim),\n",
        "            self.make_disc_block(hidden_dim, hidden_dim * 2),\n",
        "        )\n",
        "        self.d_layer = self.make_disc_block(hidden_dim * 2, 1, final_layer=True)\n",
        "        self.q_layer = nn.Sequential(\n",
        "            self.make_disc_block(hidden_dim * 2, hidden_dim * 2),\n",
        "            self.make_disc_block(hidden_dim * 2, 2 * c_dim, kernel_size=1, final_layer=True)\n",
        "        )\n",
        "\n",
        "    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
        "        '''\n",
        "        Function to return a sequence of operations corresponding to a discriminator block of the DCGAN; \n",
        "        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\n",
        "        Parameters:\n",
        "            input_channels: how many channels the input feature representation has\n",
        "            output_channels: how many channels the output feature representation should have\n",
        "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
        "            stride: the stride of the convolution\n",
        "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
        "                      (affects activation and batchnorm)\n",
        "        '''\n",
        "        if not final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "            )\n",
        "\n",
        "    def forward(self, image):\n",
        "        '''\n",
        "        Function for completing a forward pass of the discriminator: Given an image tensor, \n",
        "        returns a 1-dimension tensor representing fake/real.\n",
        "        Parameters:\n",
        "            image: a flattened image tensor with dimension (im_chan)\n",
        "        '''\n",
        "        intermediate_pred = self.disc(image)\n",
        "        disc_pred = self.d_layer(intermediate_pred)\n",
        "        q_pred = self.q_layer(intermediate_pred)\n",
        "        return disc_pred.view(len(disc_pred), -1), q_pred.view(len(q_pred), -1)"
      ],
      "metadata": {
        "id": "jBFdFywXE8B9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_vectors(x, y):\n",
        "    '''\n",
        "    Function for combining two vectors with shapes (n_samples, ?) and (n_samples, ?).\n",
        "    Parameters:\n",
        "      x: (n_samples, ?) the first vector. \n",
        "        This will be the noise vector of shape (n_samples, z_dim).\n",
        "      y: (n_samples, ?) the second vector.\n",
        "        Once again, in this assignment this will be the one-hot class vector \n",
        "        with the shape (n_samples, n_classes).\n",
        "    '''\n",
        "    combined = torch.cat([x.float(), y.float()], 1)\n",
        "    return combined"
      ],
      "metadata": {
        "id": "4yq6KqmwE--A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Let's include the same parameters from previous assignments, as well as a new `c_dim` dimension for the dimensionality of the InfoGAN latent code, a `c_criterion`, and its corresponding constant, `c_lambda`:\n",
        "\n",
        "  *   mnist_shape: the number of pixels in each MNIST image, which has dimensions 28 x 28 and one channel (because it's black-and-white) so 1 x 28 x 28\n",
        "  *   adv_criterion: the vanilla GAN loss function\n",
        "  *   c_criterion: the additional mutual information term\n",
        "  *   c_lambda: the weight on the c_criterion\n",
        "  *   n_epochs: the number of times you iterate through the entire dataset when training\n",
        "  *   z_dim: the dimension of the noise vector\n",
        "  *   c_dim: the dimension of the InfoGAN latent code\n",
        "  *   display_step: how often to display/visualize the images\n",
        "  *   batch_size: the number of images per forward/backward pass\n",
        "  *   lr: the learning rate\n",
        "  *   device: the device type\n"
      ],
      "metadata": {
        "id": "3Tofgo1rFA8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions.normal import Normal\n",
        "adv_criterion = nn.BCEWithLogitsLoss()\n",
        "c_criterion = lambda c_true, mean, logvar: Normal(mean, logvar.exp()).log_prob(c_true).mean()\n",
        "c_lambda = 0.1\n",
        "mnist_shape = (1, 28, 28)\n",
        "n_epochs = 80\n",
        "z_dim = 64\n",
        "c_dim = 2\n",
        "display_step = 500\n",
        "batch_size = 128\n",
        "# InfoGAN uses two different learning rates for the models\n",
        "d_lr = 2e-4\n",
        "g_lr = 1e-3\n",
        "device = 'cpu' # 'cuda'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "])\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    MNIST('.', download=True, transform=transform),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True)"
      ],
      "metadata": {
        "id": "vXFldROLFDNG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You initialize your networks as usual - notice that there is no separate $Q$ network. There are a few \"design\" choices worth noting here: \n",
        "1. There are many possible choices for the distribution over the latent code. You use a Gaussian prior here, but a categorical (discrete) prior is also possible, and in fact it's possible to use them together. In this case, it's also possible to use different weights $\\lambda$ on both prior distributions. \n",
        "2. You can calculate the mutual information explicitly, including $H(c)$ which you treat as constant here. You don't do that here since you're not comparing the mutual information of different parameterizations of the latent code.\n",
        "3. There are multiple ways to handle the $Q$ network - this code follows the original paper by treating it as part of the discriminator, sharing most weights, but it is also possible to simply initialize another network."
      ],
      "metadata": {
        "id": "7XIfLQKmFF9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen = Generator(input_dim=z_dim + c_dim).to(device)\n",
        "gen_opt = torch.optim.Adam(gen.parameters(), lr=g_lr)\n",
        "disc = Discriminator(im_chan=mnist_shape[0], c_dim=c_dim).to(device)\n",
        "disc_opt = torch.optim.Adam(disc.parameters(), lr=d_lr)\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "    if isinstance(m, nn.BatchNorm2d):\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias, 0)\n",
        "gen = gen.apply(weights_init)\n",
        "disc = disc.apply(weights_init)"
      ],
      "metadata": {
        "id": "4Y7DVpPvFKm0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cur_step = 0\n",
        "generator_losses = []\n",
        "discriminator_losses = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Dataloader returns the batches and the labels\n",
        "    for real, _ in tqdm(dataloader):\n",
        "        cur_batch_size = len(real)\n",
        "        # Flatten the batch of real images from the dataset\n",
        "        real = real.to(device)\n",
        "\n",
        "        c_labels = get_noise(cur_batch_size, c_dim, device=device)    \n",
        "        ### Update discriminator ###\n",
        "        # Zero out the discriminator gradients\n",
        "        disc_opt.zero_grad()\n",
        "        # Get noise corresponding to the current batch_size \n",
        "        fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
        "        # Combine the noise vectors and the one-hot labels for the generator\n",
        "        noise_and_labels = combine_vectors(fake_noise, c_labels)\n",
        "        # Generate the conditioned fake images\n",
        "        fake = gen(noise_and_labels)\n",
        "        \n",
        "        # Get the discriminator's predictions\n",
        "        disc_fake_pred, disc_q_pred = disc(fake.detach())\n",
        "        disc_q_mean = disc_q_pred[:, :c_dim]\n",
        "        disc_q_logvar = disc_q_pred[:, c_dim:]\n",
        "        mutual_information = c_criterion(c_labels, disc_q_mean, disc_q_logvar)\n",
        "        disc_real_pred, _ = disc(real)\n",
        "        disc_fake_loss = adv_criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
        "        disc_real_loss = adv_criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
        "        disc_loss = (disc_fake_loss + disc_real_loss) / 2 - c_lambda * mutual_information\n",
        "        disc_loss.backward(retain_graph=True)\n",
        "        disc_opt.step() \n",
        "\n",
        "        # Keep track of the average discriminator loss\n",
        "        discriminator_losses += [disc_loss.item()]\n",
        "\n",
        "        ### Update generator ###\n",
        "        # Zero out the generator gradients\n",
        "        gen_opt.zero_grad()\n",
        "\n",
        "        disc_fake_pred, disc_q_pred = disc(fake)\n",
        "        disc_q_mean = disc_q_pred[:, :c_dim]\n",
        "        disc_q_logvar = disc_q_pred[:, c_dim:]\n",
        "        mutual_information = c_criterion(c_labels, disc_q_mean, disc_q_logvar)\n",
        "        gen_loss = adv_criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) - c_lambda * mutual_information\n",
        "        gen_loss.backward()\n",
        "        gen_opt.step()\n",
        "\n",
        "        # Keep track of the generator losses\n",
        "        generator_losses += [gen_loss.item()]\n",
        "\n",
        "        if cur_step % display_step == 0 and cur_step > 0:\n",
        "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
        "            disc_mean = sum(discriminator_losses[-display_step:]) / display_step\n",
        "            print(f\"Epoch {epoch}, step {cur_step}: Generator loss: {gen_mean}, discriminator loss: {disc_mean}\")\n",
        "            show_tensor_images(fake)\n",
        "            show_tensor_images(real)\n",
        "            step_bins = 20\n",
        "            x_axis = sorted([i * step_bins for i in range(len(generator_losses) // step_bins)] * step_bins)\n",
        "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
        "            plt.plot(\n",
        "                range(num_examples // step_bins), \n",
        "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
        "                label=\"Generator Loss\"\n",
        "            )\n",
        "            plt.plot(\n",
        "                range(num_examples // step_bins), \n",
        "                torch.Tensor(discriminator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
        "                label=\"Discriminator Loss\"\n",
        "            )\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "        cur_step += 1"
      ],
      "metadata": {
        "id": "Krsb6TyPFSkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploration\n",
        "# Before you explore, you should put the generator\n",
        "# in eval mode, both in general and so that batch norm\n",
        "# doesn't cause you issues and is using its eval statistics\n",
        "gen = gen.eval()"
      ],
      "metadata": {
        "id": "fk2CIU1NFM0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Changing the Latent Code Vector\n",
        "You can generate some numbers with your new model! You can add interpolation as well to make it more interesting.\n",
        "\n",
        "So starting from a image, you will produce intermediate images that look more and more like the ending image until you get to the final image. Your're basically morphing one image into another. You can choose what these two images will be using your conditional GAN."
      ],
      "metadata": {
        "id": "czIDzxKMFaO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "### Change me! ###\n",
        "n_interpolation = 9 # Choose the interpolation: how many intermediate images you want + 2 (for the start and end image)\n",
        "\n",
        "def interpolate_class(n_view=5):\n",
        "    interpolation_noise = get_noise(n_view, z_dim, device=device).repeat(n_interpolation, 1)\n",
        "    first_label = get_noise(1, c_dim).repeat(n_view, 1)[None, :]\n",
        "    second_label = first_label.clone()\n",
        "    first_label[:, :, 0] =  -2\n",
        "    second_label[:, :, 0] =  2\n",
        "    \n",
        "\n",
        "    # Calculate the interpolation vector between the two labels\n",
        "    percent_second_label = torch.linspace(0, 1, n_interpolation)[:, None, None]\n",
        "    interpolation_labels = first_label * (1 - percent_second_label) + second_label * percent_second_label\n",
        "    interpolation_labels = interpolation_labels.view(-1, c_dim)\n",
        "\n",
        "    # Combine the noise and the labels\n",
        "    noise_and_labels = combine_vectors(interpolation_noise, interpolation_labels.to(device))\n",
        "    fake = gen(noise_and_labels)\n",
        "    show_tensor_images(fake, num_images=n_interpolation * n_view, nrow=n_view, show=False)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "interpolate_class()\n",
        "_ = plt.axis('off')\n"
      ],
      "metadata": {
        "id": "wOXzO81oFcg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also visualize the impact of pairwise changes of the latent code for a given noise vector.\n",
        "import math\n",
        "\n",
        "### Change me! ###\n",
        "n_interpolation = 8 # Choose the interpolation: how many intermediate images you want + 2 (for the start and end image)\n",
        "\n",
        "def interpolate_class():\n",
        "    interpolation_noise = get_noise(1, z_dim, device=device).repeat(n_interpolation * n_interpolation, 1)\n",
        "    first_label = get_noise(1, c_dim).repeat(n_interpolation * n_interpolation, 1)\n",
        "    \n",
        "    # Calculate the interpolation vector between the two labels\n",
        "    first_label = torch.linspace(-2, 2, n_interpolation).repeat(n_interpolation)\n",
        "    second_label = torch.linspace(-2, 2, n_interpolation).repeat_interleave(n_interpolation)\n",
        "    interpolation_labels = torch.stack([first_label, second_label], dim=1) \n",
        "\n",
        "    # Combine the noise and the labels\n",
        "    noise_and_labels = combine_vectors(interpolation_noise, interpolation_labels.to(device))\n",
        "    fake = gen(noise_and_labels)\n",
        "    show_tensor_images(fake, num_images=n_interpolation * n_interpolation, nrow=n_interpolation, show=False)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "interpolate_class()\n",
        "_ = plt.axis('off')\n"
      ],
      "metadata": {
        "id": "reEs4K-oFhGT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}